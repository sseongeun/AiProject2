{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba78d3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c1db04bea41d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfc7b1",
   "metadata": {},
   "source": [
    "## 1. 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7d64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 ## 이미지 사이즈를 244 x 244로 설정\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d53f2",
   "metadata": {},
   "source": [
    "## 2. 데이터 다운 및 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806c8387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data 개수: 45000\n",
      "Val data 개수: 5000\n",
      "Test data 개수: 10000\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# cifar10은 답을 포함함 \n",
    "\n",
    "## progress bar disable \n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "#분류할 클래스 개수 \n",
    "num_classes=10 # Cifar10의 클래스 개수\n",
    "\n",
    "# 훈련 데이터 (90%) , 검증 데이터 (10%) , 테스트 데이터 슬라이싱\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cifar10',\n",
    "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
    "    with_info=True,   # 메타 데이터도 불러옴\n",
    "    as_supervised=True, # 데이터셋을 (input, target) 형태로 로드하도록 지정 \n",
    "                        # (이미지, 레이블) 쌍으로 데이터셋이 반환\n",
    ")\n",
    "\n",
    "print(\"Train data 개수:\",len(raw_train))\n",
    "print(\"Val data 개수:\",len(raw_validation))\n",
    "print(\"Test data 개수:\",len(raw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc8513",
   "metadata": {},
   "source": [
    "## 3. 이미지셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3bfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 format_example 함수 정의 \n",
    "# -> row_train, raw_validation, raw_test에 적용함 \n",
    "\n",
    "# 정규화 과정의 필요성 ([-1, 1]): 신경망 모델의 학습 속도, 그래디언트 안정성, 모델의 일반화 능력 향상\n",
    "\n",
    "def format_example(image, label): \n",
    "  image = tf.cast(image, tf.float32) # 이미지 값 -> 실수형으로 표현\n",
    "  image = (image/127.5) - 1 # 이미지의 값 범위를 [-1, 1]로 정규화\n",
    "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) # 지정된 크기로 동일하게 변환\n",
    "  return image, label\n",
    "  \n",
    "##map 함수를 사용하여 데이터셋의 각 항목에 데이터 포맷 함수를 적용\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95c5b",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 만들기\n",
    " - 1. 데이터셋을 셔플함 ( 편향 방지, 일반화 능력향상, 학습 다양성 증진 )\n",
    " - 2. 배치로 나눈다. ( 위에서 배치 사이즈를 지정, 학습/검증/테스트 데이터셋 모두에 적용 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc69bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치 단위로 데이터를 처리하면\n",
    "# GPU 또는 TPU와 같은 가속기를 효과적으로 활용하여 모델의 학습과 평가 속도를 향상시킬 수 있다.\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)\n",
    "\n",
    "## train set 개수 / 배치사이즈 = len(train_batches)\n",
    "## 45000/ 128 = 352 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3364b",
   "metadata": {},
   "source": [
    "## 5. VGG16 모델 불러오기\n",
    " - 참고 \n",
    " - 'Imagent' : ImageNet 데이터셋에서 사전 훈련된 가중치는 ImageNet Challenge라고 불리는 대규모 이미지 분류 대회에서 훈련된 모델의 학습 가중치. ImageNet은 약 1,000개의 다양한 클래스에 대해 약 1,000만 개의 이미지로 구성된 대규모 데이터셋\n",
    " \n",
    " - Sequential 모델의 마지막에 Dense 레이어를 추가함. num_classes는 분류하려는 클래스의 개수를 나타내며, activation='softmax'로 설정하여 클래스별 확률 값을 출력 -> 모델의 최종 출력이 클래스에 대한 확률 분포로 나타남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48d8aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467096/553467096 [==============================] - 200s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 이미지 형태정의 /  (이미지 크기, 이미지 크기, 채널 수) 여기서는 3채널 컬러 이미지를 사용하므로 3으로 설정\n",
    "\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "#CNN 모델 변경하려면 여기서 변경\n",
    "#ImageNet으로 사전 훈련된 모델 불러오기 \n",
    "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                                ## 분류하는 부분까지 추출\n",
    "                                               include_top=True,\n",
    "                                                 ##  VGG16 모델의 완전 연결 레이어를 포함\n",
    "                                                classes=1000,\n",
    "                                                ## 이미지넷에서 사용한 가중치 적용\n",
    "                                                ## 사전 훈련된 가중치 \n",
    "                                               weights='imagenet')\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# VGG16 모델의 마지막 레이어 전까지의 레이어를 Sequential 모델에 추가\n",
    "for layer in base_model.layers[:-1]: # go through until last layer\n",
    "    model.add(layer)\n",
    "    \n",
    "    \n",
    "#마지막 layer의 최종 분류 개수를 클래스 개수와 맞게 설정\n",
    "## 활성함수로는 softmax를 사용\n",
    "model.add(keras.layers.Dense(num_classes, activation='softmax',name='predictions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b943276c",
   "metadata": {},
   "source": [
    "## 6. model_ summary\n",
    " - VGG16 모델은 일반적으로 3x3 크기의 필터, 스트라이드 1, 패딩 'same'을 사용하여 구성 활성화 함수로는 주로 ReLU(Rectified Linear Unit) 함수가 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f76b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,301,514\n",
      "Trainable params: 134,301,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45155b",
   "metadata": {},
   "source": [
    "## 7. 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622e9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c649509",
   "metadata": {},
   "source": [
    "## 8. 모델 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c92637",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_batches,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_batches,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6e304",
   "metadata": {},
   "source": [
    "## 9. test 데이터로 모델 정확도 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(test_batches, batch_size=64)\n",
    "print(\"테스트 성능 : {}%\".format(round(loss_and_metrics[1]*100,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
